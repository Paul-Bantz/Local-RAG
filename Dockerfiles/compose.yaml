services:
  llama32-3b:
    container_name: llama32-3b
    pull_policy: always
    build:
      context: .
      dockerfile: ollama.dockerfile
    ports:
      - 11434:11434
    restart: always
    stop_signal: SIGKILL
    devices:
        - nvidia.com/gpu=all
    security_opt:
        - label=disable
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['nvidia.com/gpu=0']
              capabilities: [gpu]